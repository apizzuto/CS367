/*******************************************************************************
File:             Questions.txt

Author:           Alex Pizzuto
				 pizzuto@cs.wisc.edu              

Completion Date:  4/9/2018

Course:           CS 367, Spring 2018 
*******************************************************************************/
Directions: answer the following seven (7) questions.  Note: some of the 
questions may require you to know how the LinkedList class is implemented; in 
these cases, you should assume that it is implemented as a doubly-linked chain
of nodes with references to the head and the tail.

1) The value tested by TestHash are Integer.  For an Integer storing value k, 
what is the hashCode() returned?  By trying a few values you can quickly figure 
out what it does.  

Answer: By trying a few thousand values (looping over various sets of ints and 
returning the hashCode of the Integer Object storing the value of the int), it 
is clear that the hashCode() of an Integer storing value k is just k.


2) Suppose you insert an item into your hashtable and then immediately do a 
lookup on that item.  What is the worst-case complexity of your program in this
situation?  Briefly explain your answer.

Answer: The worst case complexity of insert occurs when we must resize the 
hashtable, and this operation is O(N) where N is the number of items in the
hashtable. Then, to add an item to the end of a LinkedList (if implemented 
as a doubly-linked list with access to the head and tail node), is just O(1).
Finally, to perform the lookup, the worst case complexity is O(M) where M is
the size of the LinkedList of interest, and in the worst case this is the
maximum chain length if specified, and the number of items in the hashtable 
if not. Thus, the worst case overall time complexity is just O(N).


3) What is the worst-case complexity of your dump() method?  Briefly explain
your answer, making sure to identify the problem size(s).

Answer: In order to perform a dump(), we must iterate through the entire
hashtable and access each item in the list. While each LikedList has the 
potential to be size = number of items in the hashtable, we have the 
additional constraint that the sum of sizes of the LinkedLists is just
equal to the number of items overall. Thus, the total worst-case complexity
is just O(N) where N is the table size.


4) In this question you will run TestHash four times using the parameters 
indicated in the table:
    run1        439 323232 10.0 60 0 filename
    run2        439 323232 10.0 61 0 filename
    run3        439 323232 10.0 60 0 filename
    run4        439 323232 10.0 61 0 filename
where filename can be any file you want.  
- For run1 and run2 you will use the TestHash program as written.  
- For run3 and run4 you will modify the TestHash program so that instead of 
  inserting random integers into the hashtable, it will insert the numbers 
  0, 3, 6, 9, 12, ...
  The easiest way to do this is to replace the line:
      table.insert(new Integer(randGen.nextInt()));
  with the line:
      table.insert(new Integer(3*k)); 

What are the hashtable statistics for each of the runs?  

Answer:
Run1 Hashtable statistics:
  current table size:		60
  # items in table:			439
  current load factor:		7.316666666666666
  longest chain length:		14
  # 0-length chains:			0
  avg (non-0) chain length:	7.316666666666666
  
Run 2 Hashtable statistics:
  current table size:		61
  # items in table:			439
  current load factor:		7.19672131147541
  longest chain length:		14
  # 0-length chains:			0
  avg (non-0) chain length:	7.19672131147541
  
Run 3 Hashtable statistics:
  current table size:		60
  # items in table:			439
  current load factor:		7.316666666666666
  longest chain length:		22
  # 0-length chains:			40
  avg (non-0) chain length:	21.95
  
Run 4 Hashtable statistics:
  current table size:		61
  # items in table:			439
  current load factor:		7.19672131147541
  longest chain length:		8
  # 0-length chains:			0
  avg (non-0) chain length:	7.19672131147541

5) In this question you will again run TestHash four times, this time using the 
parameters:
    run5        439 323232 0.8 60 0 filename
    run6        439 323232 0.8 61 0 filename
    run7        439 323232 0.8 60 0 filename
    run8        439 323232 0.8 61 0 filename
where filename can be any file you want.  
- For run5 and run6 you will use the TestHash program as written.  
- For run7 and run8 you will modify the TestHash program so that instead of
  inserting random integers into the hashtable, it will insert the numbers 
  0, 3, 6, 9, 12, ...
  The easiest way to do this is to replace the line:
      table.insert(new Integer(randGen.nextInt()));
  with the line:
      table.insert(new Integer(3*k)); 

What are the hashtable statistics for each of the runs?  

Answer:
Run 5 Hashtable statistics:
  current table size:		975
  # items in table:			439
  current load factor:		0.4502564102564103
  longest chain length:		4
  # 0-length chains:			616
  avg (non-0) chain length:	1.222841225626741
  
Run 6 Hashtable statistics:
  current table size:		991
  # items in table:			439
  current load factor:		0.4429868819374369
  longest chain length:		4
  # 0-length chains:			638
  avg (non-0) chain length:	1.2436260623229463
  
Run 7 Hashtable statistics:
  current table size:		975
  # items in table:			439
  current load factor:		0.4502564102564103
  longest chain length:		2
  # 0-length chains:			650
  avg (non-0) chain length:	1.3507692307692307
  
Run 8 Hashtable statistics:
  current table size:		991
  # items in table:			439
  current load factor:		0.4429868819374369
  longest chain length:		1
  # 0-length chains:			552
  avg (non-0) chain length:	1.0


6) In this question you will again run TestHash four times, this time using the 
parameters:
    run9        439 323232 10.0 11 0 filename
    run10       439 323232 10.0 11 5 filename
    run11       439 323232 0.8 11 0 filename
    run12       439 323232 0.8 11 5 filename
where filename can be any file you want.  
For all these runs you will use the TestHash program as written.  

What are the hashtable statistics for each of the runs?  

Answer:
Run 9 Hashtable statistics:
  current table size:		47
  # items in table:			439
  current load factor:		9.340425531914894
  longest chain length:		17
  # 0-length chains:			0
  avg (non-0) chain length:	9.340425531914894
  
Run 10 Hashtable statistics:
  current table size:		767
  # items in table:			439
  current load factor:		0.5723598435462842
  longest chain length:		4
  # 0-length chains:			429
  avg (non-0) chain length:	1.2988165680473374
  
Run 11 Hashtable statistics:
  current table size:		767
  # items in table:			439
  current load factor:		0.5723598435462842
  longest chain length:		4
  # 0-length chains:			429
  avg (non-0) chain length:	1.2988165680473374
  
Run 12 Hashtable statistics:
  current table size:		767
  # items in table:			439
  current load factor:		0.5723598435462842
  longest chain length:		4
  # 0-length chains:			429
  avg (non-0) chain length:	1.2988165680473374


7) Briefly analyze your results from questions 4, 5, and 6.  Your analysis 
should consider the following (in relation to the final distribution of the 
hashtable):
    - the characteristics of the table size (i.e., prime or non-prime)
    - the characteristics of the input data 
    - the load factor (i.e., is the hashtable resized or not?)
    - the maximum chain length (i.e., whether or not there is a max)
and describe how they are related.

Answer: Various factors have an effect on the efficacy of using this 
implementation of a HashTable. First, if the table size is prime to begin,
then there seem to be smaller average chain lengths and fewer empty
chains. The uniqueness of prime factorization helps to ensure this
so as to have a mapping that is more bijective instead of just mapping
to a subset of the codomain (in this case, modular hash values). This is
seen especially when comparing run3 to run4, run7 to run8, and run9 to run10. 
In all of these cases, the tables with initially prime sizes have less empty
chains and smaller longest chains. Another factor that plays a large part is if
the input data is random or not random. If the data is random, then prime and 
composite initial table lengths tend to perform similarly (ie run1 to run2, run5
to run6). However, if the input is not random, then the prime initial size performs
much better than the composite initial size (this is especially clear when 
comparing the longest chain length of 22 to the chain length of 8 from run4 to
run3, it is also somewhat noticeable from run8 to run7). Another factor is the 
load factor. If the load factor is increased, then the final tablesize seems
to be much smaller (comparing all runs from question 4 to those in question 5). 
Also, if the load factor is decreased, then the longest chain length and 
average chain length are much shorter (again, comparing all runs from 
question 4 to those of question 5). Finally, the maximum chain length
has noticeable effects. If a maximum chain length is specified (or reduced),
then not only do the average and longest chain lengths decrease, but the table
also has to resize more, causing a larger ending table size (seen in all iterations
for question 4 and question 5).
